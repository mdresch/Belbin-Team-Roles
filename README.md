# Belbin-Team-Roles
Belbin Team Roles https://cba-teams.hashnode.space/

# Belbin Role and Sentiment Analysis of Meeting Contributions

This project analyzes sentences from team meeting contributions, classifying them according to:

1.  **Category:** Action, Thought, or People (using the Google Gemini API).
2.  **Belbin Role:** Shaper, Implementer, Completer Finisher, Co-ordinator, Teamworker, Resource Investigator, Plant, Monitor Evaluator, or Specialist (using the Google Gemini API).
3.  **General Sentiment:** Positive, Neutral, or Negative (using a pre-trained DistilBERT model from Hugging Face).
4.  **Belbin-Specific Sentiment:** Positive, Neutral, or Negative *within the context of the identified Belbin role* (using a fine-tuned DistilBERT model).

The project is structured into several Python files, each with a specific purpose. This modular design makes the code easier to understand, maintain, and extend.

## Project Structure

*   **`config.py`:** Contains all configuration variables (API keys, model names, file paths, hyperparameters).
*   **`prompts.py`:** Stores the prompts used for the Gemini API calls (category and Belbin role classification).
*   **`utils.py`:**  Contains utility functions for:
    *   Calling the Gemini API.
    *   Parsing JSON responses.
    *   Loading the fine-tuned sentiment analysis model.
    *   Performing general sentiment analysis (using the pre-trained DistilBERT).
    *   Performing Belbin-specific sentiment analysis (using the fine-tuned DistilBERT).
    *   Performing category classification (using Gemini).
    *   Performing Belbin role classification (using Gemini).
*   **`main.py`:** The main script that orchestrates the entire analysis process.  It loads data, calls the analysis functions, and outputs the results.
*   **`train.py`:** Contains the code to fine-tune the DistilBERT model for Belbin-specific sentiment analysis.
*   **`create_csv.py`:** A utility script to create the `sentences.csv` input file from a multi-line string. This avoids manual CSV editing.
*   **`prepare_data.py`:** Loads the data from `sentences.csv`, preprocesses it, and splits it into training and testing sets.
*   **`sentences.csv`:**  The input data file (generated by `create_csv.py`).  Contains sentences, Belbin roles, and role-specific sentiment labels.  Format: `sentence,belbin_role,label` (no header row).
*   **`train.csv`:** The training data, created by `prepare_data.py`.
*   **`test.csv`:** The testing data, created by `prepare_data.py`.
* **`validation.csv`:**. The validation set.
*   **`fine_tuned_model/`:** A directory (created by `train.py`) where the fine-tuned model, tokenizer, and label map are saved.
*    **`results/`**: Created when training. Will contain the training results.
*    **`logs/`**: Created when training. Logs are saved here.
*   **`meeting_analysis_results_belbin_only.csv`:** The output CSV file, containing the analysis results.

## Setup and Installation

1.  **Clone the Repository (if applicable):** If the code is hosted on a repository (like GitHub), clone it to your local machine:

    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```

2.  **Create a Virtual Environment (Recommended):**

    ```bash
    python3 -m venv venv
    # Activate on Windows:
    venv\Scripts\activate
    # Activate on macOS/Linux:
    source venv/bin/activate
    ```

3.  **Install Dependencies:**

    ```bash
    pip install pandas google-generativeai transformers torch scikit-learn
    ```

    *   **Note:** Ensure you have a compatible version of PyTorch installed for your system (CPU or GPU, and the correct CUDA version if using a GPU).  See the PyTorch website ([https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)) for detailed instructions.

4.  **Set up `config.py`:**
    *   Create a file named `config.py` in the project directory.
    *   Copy the following code into `config.py` and *replace* `"YOUR_API_KEY"` with your actual Google Gemini API key:

    ```python
    # config.py

    GOOGLE_API_KEY = "YOUR_API_KEY"  # REPLACE WITH YOUR ACTUAL API KEY
    GOOGLE_MODEL_NAME = "gemini-pro"
    OUTPUT_CSV_PATH = "./meeting_analysis_results_belbin_only.csv"
    LOG_LEVEL = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
    DELAY_SECONDS = 2
    MAX_RETRIES = 3

    # --- Configuration for Fine-Tuning ---
    TRAIN_DATA_PATH = "train.csv"  # Path to your training data CSV
    VALIDATION_DATA_PATH = "validation.csv"  # Path to your validation data CSV. Not used in this version.
    TEST_DATA_PATH = "test.csv" # Path to your test data CSV
    FINE_TUNED_MODEL_PATH = "./fine_tuned_model"  # Where to save the fine-tuned model
    BASE_MODEL_NAME = "distilbert-base-uncased"  # Base model for fine-tuning
    NUM_LABELS = 27 #Update to the number of labels.
    LEARNING_RATE = 2e-5
    NUM_EPOCHS = 5 # was 3
    BATCH_SIZE = 16  # Adjust based on your GPU memory
    ```
    * You can adjust the other configuration variables as needed.

## Usage

The typical workflow is as follows:

1.  **Prepare Input Data (`create_csv.py`):**
    *   Open `create_csv.py` in a text editor.
    *   Paste your input data (sentences, Belbin roles, and role-specific sentiments) into the `data_string` variable.  The input format should be the multi-line string format as provided, including an ID, sentence, category, role, sentiment and keywords, separated by semicolons. The script takes care of any inconsistencies in the data.
        * Each line *must* follow the format: `ID sentence;category;role;Belbin Role Sentiment;keywords`
        * The script will skip any lines where the values aren't valid.
    *   Save the changes to `create_csv.py`.
    *   Run `create_csv.py`:

        ```bash
        python create_csv.py
        ```

        This will create (or overwrite) the `sentences.csv` file in your project directory.

2.  **Prepare Data Splits (`prepare_data.py`):**

    ```bash
    python prepare_data.py
    ```

    This script loads the `sentences.csv` file, cleans the data (removes duplicates, handles whitespace, checks for valid labels and roles), and splits it into `train.csv` and `test.csv`.

3.  **Fine-Tune the Model (`train.py`):**

    ```bash
    python train.py
    ```

    This script fine-tunes the DistilBERT model on your `train.csv` data. This step is crucial for adapting the model to your specific data and achieving good performance. It saves the fine-tuned model, tokenizer, and label mapping to the `FINE_TUNED_MODEL_PATH` directory (default: `./fine_tuned_model`).

4.  **Run the Analysis (`main.py`):**

    ```bash
    python main.py
    ```

    This script performs the complete analysis:

    *   Loads the fine-tuned model (for Belbin-specific sentiment) and the original DistilBERT model (for general sentiment).
    *   Uses the Gemini API for category and Belbin role classification.
    *   Prints the results to the console.
    *   Saves the results to a CSV file (specified by `OUTPUT_CSV_PATH` in `config.py`).

## File Descriptions

*   **`config.py`:**  Configuration variables.
*   **`prompts.py`:** Gemini API prompts.
*   **`utils.py`:** Utility functions for API calls, JSON parsing, and model loading.
*   **`main.py`:** Main analysis script.
*   **`train.py`:** Fine-tuning script.
*   **`create_csv.py`:**  Creates the `sentences.csv` input file.
*   **`prepare_data.py`:**  Prepares the data (splits into train/test).
*   **`sentences.csv`:** Input data (created by `create_csv.py`).
*   **`train.csv`:** Training data (created by `prepare_data.py`).
* **`validation.csv`:** Validation data (created by `prepare_data.py`).
*   **`test.csv`:** Test data (created by `prepare_data.py`).
*   **`fine_tuned_model/`:** Directory containing the fine-tuned model, tokenizer, and label map.
*   **`meeting_analysis_results_belbin_only.csv`:** Output CSV file with the analysis results.

## Troubleshooting
If the accuracy is low check the sentences.csv for correct labeling.
Try increasing/decreasing the neutral_threshold.
If this does not help add additional sentences to the training data.
